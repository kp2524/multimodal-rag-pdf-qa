**ğŸ¤– Multimodal PDF Question Answering System (RAG + LangChain + Gradio)**

This project is a multimodal Retrieval-Augmented Generation (RAG) pipeline for querying PDF documents that may contain text, images, and tables.
It integrates LangChain, OpenAI LLMs, and Gradio to provide an interactive interface where users can upload PDFs and ask natural language questions â€” and the system retrieves and reasons over multiple modalities (text + images + tables).

**ğŸš€ Features**

**ğŸ“„ Text Understanding** â€“ Extracts and chunks text using LangChain.

**ğŸ–¼ï¸ Image Extraction** â€“ Extracts images and allows reference in answers.

**ğŸ“Š Table Extraction** â€“ Converts detected tables into structured Pandas DataFrames.

**ğŸ” Vector Search with Query Expansion** â€“ Embeds text into FAISS and enhances queries for improved retrieval accuracy.

**ğŸ§  Retrieval-Augmented Generation (RAG)** â€“ Uses LLMs with contextual grounding from text + metadata (images & tables).

**ğŸ›ï¸ Interactive UI** â€“ Powered by Gradio for uploading PDFs and asking questions in real time.

**ğŸ› ï¸ Tech Stack**

Python 3.10+

LangChain (chains, prompts, embeddings, RAG pipeline)

OpenAI API (ChatOpenAI + Embeddings)

FAISS (Vector Database)

PyMuPDF (fitz) â€“ Image & table extraction

PyPDF2 â€“ Text extraction

Pytesseract â€“ OCR support for scanned images

Pandas â€“ Table structuring

Gradio â€“ Frontend interface



**âš™ï¸ Setup Instructions**

1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

**2ï¸âƒ£ Create Virtual Environment & Install Dependencies**
python -m venv venv
source venv/bin/activate   # For Linux/Mac
venv\Scripts\activate      # For Windows

pip install -r requirements.txt

**3ï¸âƒ£ Configure Environment Variables**

Create a .env file in the project root:

OPENAI_API_KEY=your_api_key_here

**â–¶ï¸ Run the Application**
python project.py


**The app will start at:**
ğŸ‘‰ http://127.0.0.1:7860

**ğŸ’» Usage**

Upload a PDF file (supports text, images, and tables)

Enter your question in natural language

The system will:

Expand your query with related terms

Retrieve relevant text chunks and metadata about images/tables

Generate a concise, multimodal-aware answer using the LLM

View the answer and processing logs in real time

**ğŸ“Š Example**

Upload PDF: Research paper with figures & tables

Question: "What does Table 2 indicate about accuracy?"

Answer: Refers directly to Table 2 in the PDF with summarized insights

Log: Shows query expansion, retrieved chunks, and multimodal references
